# Natural-Language-Processing-in-TensorFlow

#### Tokenizer handles the heavy lifting for us, generating the dictionary of word encodings and creating vectors out of the sentences.

#### json.loads() => used to parse a valid JSON string and convert it into a Python Dictionary

#### json.dumps() => converts a Python object into a json string

#### flatten() and GlobalAveragePooling1D() these are usual or regular processing without any special neural network technique.

#### In natural language processing (NLP), Word Embedding is a term used for the representation of words for text analysis, typically in the formof a real-valued vector that encodes the meaning of the word such that the words that are closer in the vector space are expected to be similar in meaning.

#### In other words, Word Embedding is Word Vectorization.

#### In cases, GlobalAveragePooling1D() creates much uniform output than flatten(). So, it's better to use GAP1D() instead of flatten() in case of NLP.




































